2018-11-19 14:08:37 SparkContext: INFO: Running Spark version 2.2.0
2018-11-19 14:08:37 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-11-19 14:08:37 SparkContext: INFO: Submitted application: Hail
2018-11-19 14:08:37 SparkContext: INFO: Spark configuration:
spark.app.name=Hail
spark.driver.extraClassPath=/anaconda3/lib/python3.6/site-packages/hail/hail-all-spark.jar
spark.executor.extraClassPath=/anaconda3/lib/python3.6/site-packages/hail/hail-all-spark.jar
spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576
spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
spark.logConf=true
spark.master=local[*]
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.submit.deployMode=client
spark.ui.showConsoleProgress=false
2018-11-19 14:08:37 SecurityManager: INFO: Changing view acls to: nbaya
2018-11-19 14:08:37 SecurityManager: INFO: Changing modify acls to: nbaya
2018-11-19 14:08:37 SecurityManager: INFO: Changing view acls groups to: 
2018-11-19 14:08:37 SecurityManager: INFO: Changing modify acls groups to: 
2018-11-19 14:08:37 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nbaya); groups with view permissions: Set(); users  with modify permissions: Set(nbaya); groups with modify permissions: Set()
2018-11-19 14:08:37 Utils: INFO: Successfully started service 'sparkDriver' on port 51668.
2018-11-19 14:08:37 SparkEnv: INFO: Registering MapOutputTracker
2018-11-19 14:08:38 SparkEnv: INFO: Registering BlockManagerMaster
2018-11-19 14:08:38 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-11-19 14:08:38 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up
2018-11-19 14:08:38 DiskBlockManager: INFO: Created local directory at /private/var/folders/56/q7rh2y5x7hj4n359_13dgwxcmtwz20/T/blockmgr-184cc326-6058-41eb-83e4-8a768380d8bf
2018-11-19 14:08:38 MemoryStore: INFO: MemoryStore started with capacity 366.3 MB
2018-11-19 14:08:38 SparkEnv: INFO: Registering OutputCommitCoordinator
2018-11-19 14:08:38 log: INFO: Logging initialized @1753ms
2018-11-19 14:08:38 Server: INFO: jetty-9.3.z-SNAPSHOT
2018-11-19 14:08:38 Server: INFO: Started @1823ms
2018-11-19 14:08:38 Utils: WARN: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2018-11-19 14:08:38 AbstractConnector: INFO: Started ServerConnector@f70b44e{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2018-11-19 14:08:38 Utils: INFO: Successfully started service 'SparkUI' on port 4041.
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@125715a0{/jobs,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35ed8cfb{/jobs/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@6810d5bf{/jobs/job,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@151137c8{/jobs/job/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5e3a3083{/stages,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@202aa359{/stages/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@419830af{/stages/stage,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4019feb6{/stages/stage/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1f6c9a5c{/stages/pool,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75740171{/stages/pool/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@49b2155d{/storage,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5baefe7c{/storage/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@e2dbb9e{/storage/rdd,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@c3c1a4d{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5ad12028{/environment,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7c3d7c01{/environment/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@a36d088{/executors,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@723d6016{/executors/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3c2b6de0{/executors/threadDump,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7e8d6fb6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@33de36b6{/static,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@4abc2ab8{/,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@65b65942{/api,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9aea85{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@52277258{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.0.0.51:4041
2018-11-19 14:08:38 Executor: INFO: Starting executor ID driver on host localhost
2018-11-19 14:08:38 Utils: INFO: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51669.
2018-11-19 14:08:38 NettyBlockTransferService: INFO: Server created on 10.0.0.51:51669
2018-11-19 14:08:38 BlockManager: INFO: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-11-19 14:08:38 BlockManagerMaster: INFO: Registering BlockManager BlockManagerId(driver, 10.0.0.51, 51669, None)
2018-11-19 14:08:38 BlockManagerMasterEndpoint: INFO: Registering block manager 10.0.0.51:51669 with 366.3 MB RAM, BlockManagerId(driver, 10.0.0.51, 51669, None)
2018-11-19 14:08:38 BlockManagerMaster: INFO: Registered BlockManager BlockManagerId(driver, 10.0.0.51, 51669, None)
2018-11-19 14:08:38 BlockManager: INFO: Initialized BlockManager: BlockManagerId(driver, 10.0.0.51, 51669, None)
2018-11-19 14:08:38 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@77fc5945{/metrics/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:38 Hail: INFO: SparkUI: http://10.0.0.51:4041
2018-11-19 14:08:38 Hail: INFO: Running Hail version 0.2-a2eaf89baa0c
2018-11-19 14:08:39 SharedState: INFO: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/nbaya/Documents/lab/ldscsim/spark-warehouse/').
2018-11-19 14:08:39 SharedState: INFO: Warehouse path is 'file:/Users/nbaya/Documents/lab/ldscsim/spark-warehouse/'.
2018-11-19 14:08:39 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@72aa6438{/SQL,null,AVAILABLE,@Spark}
2018-11-19 14:08:39 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@21351659{/SQL/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:39 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@215ca3fd{/SQL/execution,null,AVAILABLE,@Spark}
2018-11-19 14:08:39 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5f204517{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-11-19 14:08:39 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@2408e7d3{/static/sql,null,AVAILABLE,@Spark}
2018-11-19 14:08:39 StateStoreCoordinatorRef: INFO: Registered StateStoreCoordinator endpoint
2018-11-19 14:09:40 root: INFO: optimize: before:
(TableKeyBy (`0`) False
  (TableMapRows
    (TableKeyBy () False
      (TableKeyBy () False
        (TableLiteral)))
    (InsertFields
      (Ref row)
      (0
        (GetField `0`
          (Ref row))))))
2018-11-19 14:09:40 root: INFO: optimize: after:
(TableKeyBy (`0`) False
  (TableMapRows
    (TableLiteral)
    (InsertFields
      (Ref row)
      (0
        (GetField `0`
          (Ref row))))))
2018-11-19 14:09:40 root: INFO: optimize: before:
(InsertFields
  (Ref global))
2018-11-19 14:09:40 root: INFO: optimize: after:
(Ref global)
2018-11-19 14:09:40 MemoryStore: INFO: Block broadcast_0 stored as values in memory (estimated size 56.0 B, free 366.3 MB)
2018-11-19 14:09:40 MemoryStore: INFO: Block broadcast_0_piece0 stored as bytes in memory (estimated size 67.0 B, free 366.3 MB)
2018-11-19 14:09:40 BlockManagerInfo: INFO: Added broadcast_0_piece0 in memory on 10.0.0.51:51669 (size: 67.0 B, free: 366.3 MB)
2018-11-19 14:09:40 SparkContext: INFO: Created broadcast 0 from broadcast at BroadcastValue.scala:14
2018-11-19 14:09:40 root: INFO: initop (Begin)
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C0.<init> instruction count: 3
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 15
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 16
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C0.setPartitionIndex instruction count: 14
2018-11-19 14:09:40 root: INFO: seqop (Begin)
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C1.<init> instruction count: 3
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 15
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C1.apply instruction count: 22
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C1.setPartitionIndex instruction count: 14
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C2.<init> instruction count: 3
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C2.apply instruction count: 79
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C2.apply instruction count: 28
2018-11-19 14:09:40 root: INFO: is/hail/codegen/generated/C2.setPartitionIndex instruction count: 14
2018-11-19 14:09:40 MemoryStore: INFO: Block broadcast_1 stored as values in memory (estimated size 1640.0 B, free 366.3 MB)
2018-11-19 14:09:40 MemoryStore: INFO: Block broadcast_1_piece0 stored as bytes in memory (estimated size 380.0 B, free 366.3 MB)
2018-11-19 14:09:40 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on 10.0.0.51:51669 (size: 380.0 B, free: 366.3 MB)
2018-11-19 14:09:40 SparkContext: INFO: Created broadcast 1 from broadcast at RVDPartitioner.scala:79
2018-11-19 14:09:40 root: INFO: optimize: before:
(SelectFields ()
  (Ref global))
2018-11-19 14:09:40 root: INFO: optimize: after:
(SelectFields ()
  (Ref global))
2018-11-19 14:09:40 SparkContext: INFO: Starting job: collect at ContextRDD.scala:139
2018-11-19 14:09:40 DAGScheduler: INFO: Got job 0 (collect at ContextRDD.scala:139) with 8 output partitions
2018-11-19 14:09:40 DAGScheduler: INFO: Final stage: ResultStage 0 (collect at ContextRDD.scala:139)
2018-11-19 14:09:40 DAGScheduler: INFO: Parents of final stage: List()
2018-11-19 14:09:40 DAGScheduler: INFO: Missing parents: List()
2018-11-19 14:09:40 DAGScheduler: INFO: Submitting ResultStage 0 (MapPartitionsRDD[14] at mapPartitions at ContextRDD.scala:133), which has no missing parents
2018-11-19 14:09:40 MemoryStore: INFO: Block broadcast_2 stored as values in memory (estimated size 19.2 KB, free 366.3 MB)
2018-11-19 14:09:40 MemoryStore: INFO: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KB, free 366.3 MB)
2018-11-19 14:09:40 BlockManagerInfo: INFO: Added broadcast_2_piece0 in memory on 10.0.0.51:51669 (size: 10.3 KB, free: 366.3 MB)
2018-11-19 14:09:40 SparkContext: INFO: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-11-19 14:09:40 DAGScheduler: INFO: Submitting 8 missing tasks from ResultStage 0 (MapPartitionsRDD[14] at mapPartitions at ContextRDD.scala:133) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
2018-11-19 14:09:40 TaskSchedulerImpl: INFO: Adding task set 0.0 with 8 tasks
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 TaskSetManager: INFO: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 4741 bytes)
2018-11-19 14:09:40 Executor: INFO: Running task 4.0 in stage 0.0 (TID 4)
2018-11-19 14:09:40 Executor: INFO: Running task 7.0 in stage 0.0 (TID 7)
2018-11-19 14:09:40 Executor: INFO: Running task 6.0 in stage 0.0 (TID 6)
2018-11-19 14:09:40 Executor: INFO: Running task 1.0 in stage 0.0 (TID 1)
2018-11-19 14:09:40 Executor: INFO: Running task 2.0 in stage 0.0 (TID 2)
2018-11-19 14:09:40 Executor: INFO: Running task 0.0 in stage 0.0 (TID 0)
2018-11-19 14:09:40 Executor: INFO: Running task 5.0 in stage 0.0 (TID 5)
2018-11-19 14:09:40 Executor: INFO: Running task 3.0 in stage 0.0 (TID 3)
2018-11-19 14:09:41 CodeGenerator: INFO: Code generated in 144.36836 ms
2018-11-19 14:09:41 CodeGenerator: INFO: Code generated in 10.067165 ms
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 474, boot = 465, init = 8, finish = 1
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 462, boot = 451, init = 11, finish = 0
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 472, boot = 456, init = 16, finish = 0
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 443, boot = 441, init = 2, finish = 0
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 437, boot = 431, init = 6, finish = 0
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 439, boot = 437, init = 2, finish = 0
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 439, boot = 429, init = 10, finish = 0
2018-11-19 14:09:41 PythonRunner: INFO: Times: total = 449, boot = 446, init = 2, finish = 1
2018-11-19 14:09:41 Executor: INFO: Finished task 2.0 in stage 0.0 (TID 2). 1613 bytes result sent to driver
2018-11-19 14:09:41 Executor: INFO: Finished task 5.0 in stage 0.0 (TID 5). 1613 bytes result sent to driver
2018-11-19 14:09:41 Executor: INFO: Finished task 7.0 in stage 0.0 (TID 7). 1613 bytes result sent to driver
2018-11-19 14:09:41 Executor: INFO: Finished task 0.0 in stage 0.0 (TID 0). 1656 bytes result sent to driver
2018-11-19 14:09:41 Executor: INFO: Finished task 1.0 in stage 0.0 (TID 1). 1656 bytes result sent to driver
2018-11-19 14:09:41 Executor: INFO: Finished task 4.0 in stage 0.0 (TID 4). 1613 bytes result sent to driver
2018-11-19 14:09:41 Executor: INFO: Finished task 3.0 in stage 0.0 (TID 3). 1613 bytes result sent to driver
2018-11-19 14:09:41 Executor: INFO: Finished task 6.0 in stage 0.0 (TID 6). 1613 bytes result sent to driver
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 5.0 in stage 0.0 (TID 5) in 903 ms on localhost (executor driver) (1/8)
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 7.0 in stage 0.0 (TID 7) in 899 ms on localhost (executor driver) (2/8)
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 2.0 in stage 0.0 (TID 2) in 916 ms on localhost (executor driver) (3/8)
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 1.0 in stage 0.0 (TID 1) in 920 ms on localhost (executor driver) (4/8)
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 3.0 in stage 0.0 (TID 3) in 913 ms on localhost (executor driver) (5/8)
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 6.0 in stage 0.0 (TID 6) in 904 ms on localhost (executor driver) (6/8)
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 4.0 in stage 0.0 (TID 4) in 910 ms on localhost (executor driver) (7/8)
2018-11-19 14:09:41 TaskSetManager: INFO: Finished task 0.0 in stage 0.0 (TID 0) in 937 ms on localhost (executor driver) (8/8)
2018-11-19 14:09:41 TaskSchedulerImpl: INFO: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-11-19 14:09:41 DAGScheduler: INFO: ResultStage 0 (collect at ContextRDD.scala:139) finished in 0.956 s
2018-11-19 14:09:41 DAGScheduler: INFO: Job 0 finished: collect at ContextRDD.scala:139, took 1.040281 s
2018-11-19 14:09:41 Hail: INFO: Coerced sorted dataset
2018-11-19 14:09:41 Hail: INFO: Coerced dataset with out-of-order partitions.
2018-11-19 14:09:41 MemoryStore: INFO: Block broadcast_3 stored as values in memory (estimated size 2.3 KB, free 366.3 MB)
2018-11-19 14:09:41 MemoryStore: INFO: Block broadcast_3_piece0 stored as bytes in memory (estimated size 557.0 B, free 366.3 MB)
2018-11-19 14:09:41 BlockManagerInfo: INFO: Added broadcast_3_piece0 in memory on 10.0.0.51:51669 (size: 557.0 B, free: 366.3 MB)
2018-11-19 14:09:41 SparkContext: INFO: Created broadcast 3 from broadcast at RVDPartitioner.scala:79
2018-11-19 14:09:41 MemoryStore: INFO: Block broadcast_4 stored as values in memory (estimated size 608.0 B, free 366.3 MB)
2018-11-19 14:09:41 MemoryStore: INFO: Block broadcast_4_piece0 stored as bytes in memory (estimated size 200.0 B, free 366.3 MB)
2018-11-19 14:09:41 BlockManagerInfo: INFO: Added broadcast_4_piece0 in memory on 10.0.0.51:51669 (size: 200.0 B, free: 366.3 MB)
2018-11-19 14:09:41 SparkContext: INFO: Created broadcast 4 from broadcast at RepartitionedOrderedRDD2.scala:37
2018-11-19 14:09:41 MemoryStore: INFO: Block broadcast_5 stored as values in memory (estimated size 1320.0 B, free 366.3 MB)
2018-11-19 14:09:41 MemoryStore: INFO: Block broadcast_5_piece0 stored as bytes in memory (estimated size 479.0 B, free 366.3 MB)
2018-11-19 14:09:41 BlockManagerInfo: INFO: Added broadcast_5_piece0 in memory on 10.0.0.51:51669 (size: 479.0 B, free: 366.3 MB)
2018-11-19 14:09:41 SparkContext: INFO: Created broadcast 5 from broadcast at RVDPartitioner.scala:79
2018-11-19 14:10:32 root: INFO: optimize: before:
(TableCount
  (MatrixRowsTable
    (MatrixLiteral)))
2018-11-19 14:10:32 root: INFO: optimize: after:
(TableCount
  (MatrixRowsTable
    (MatrixLiteral)))
2018-11-19 14:10:32 MemoryStore: INFO: Block broadcast_6 stored as values in memory (estimated size 1320.0 B, free 366.3 MB)
2018-11-19 14:10:32 MemoryStore: INFO: Block broadcast_6_piece0 stored as bytes in memory (estimated size 479.0 B, free 366.3 MB)
2018-11-19 14:10:32 BlockManagerInfo: INFO: Added broadcast_6_piece0 in memory on 10.0.0.51:51669 (size: 479.0 B, free: 366.3 MB)
2018-11-19 14:10:32 SparkContext: INFO: Created broadcast 6 from broadcast at RVDPartitioner.scala:79
2018-11-19 14:10:32 SparkContext: INFO: Starting job: fold at RVD.scala:602
2018-11-19 14:10:32 DAGScheduler: INFO: Got job 1 (fold at RVD.scala:602) with 3 output partitions
2018-11-19 14:10:32 DAGScheduler: INFO: Final stage: ResultStage 1 (fold at RVD.scala:602)
2018-11-19 14:10:32 DAGScheduler: INFO: Parents of final stage: List()
2018-11-19 14:10:32 DAGScheduler: INFO: Missing parents: List()
2018-11-19 14:10:32 DAGScheduler: INFO: Submitting ResultStage 1 (MapPartitionsRDD[25] at mapPartitions at ContextRDD.scala:133), which has no missing parents
2018-11-19 14:10:32 MemoryStore: INFO: Block broadcast_7 stored as values in memory (estimated size 25.4 KB, free 366.2 MB)
2018-11-19 14:10:32 MemoryStore: INFO: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KB, free 366.2 MB)
2018-11-19 14:10:32 BlockManagerInfo: INFO: Added broadcast_7_piece0 in memory on 10.0.0.51:51669 (size: 12.7 KB, free: 366.3 MB)
2018-11-19 14:10:32 SparkContext: INFO: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
2018-11-19 14:10:32 DAGScheduler: INFO: Submitting 3 missing tasks from ResultStage 1 (MapPartitionsRDD[25] at mapPartitions at ContextRDD.scala:133) (first 15 tasks are for partitions Vector(0, 1, 2))
2018-11-19 14:10:32 TaskSchedulerImpl: INFO: Adding task set 1.0 with 3 tasks
2018-11-19 14:10:32 TaskSetManager: INFO: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5740 bytes)
2018-11-19 14:10:32 TaskSetManager: INFO: Starting task 1.0 in stage 1.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5639 bytes)
2018-11-19 14:10:32 TaskSetManager: INFO: Starting task 2.0 in stage 1.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 5437 bytes)
2018-11-19 14:10:32 Executor: INFO: Running task 1.0 in stage 1.0 (TID 9)
2018-11-19 14:10:32 Executor: INFO: Running task 0.0 in stage 1.0 (TID 8)
2018-11-19 14:10:32 Executor: INFO: Running task 2.0 in stage 1.0 (TID 10)
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 2, boot = -51220, init = 51222, finish = 0
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 1, boot = -51209, init = 51210, finish = 0
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 1, boot = -51193, init = 51194, finish = 0
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 2, boot = -51228, init = 51229, finish = 1
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 1, boot = -51226, init = 51227, finish = 0
2018-11-19 14:10:32 Executor: INFO: Finished task 2.0 in stage 1.0 (TID 10). 1455 bytes result sent to driver
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 2, boot = -51214, init = 51215, finish = 1
2018-11-19 14:10:32 TaskSetManager: INFO: Finished task 2.0 in stage 1.0 (TID 10) in 39 ms on localhost (executor driver) (1/3)
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 4, boot = -51237, init = 51241, finish = 0
2018-11-19 14:10:32 Executor: INFO: Finished task 1.0 in stage 1.0 (TID 9). 1455 bytes result sent to driver
2018-11-19 14:10:32 TaskSetManager: INFO: Finished task 1.0 in stage 1.0 (TID 9) in 54 ms on localhost (executor driver) (2/3)
2018-11-19 14:10:32 PythonRunner: INFO: Times: total = 1, boot = -51219, init = 51220, finish = 0
2018-11-19 14:10:32 Executor: INFO: Finished task 0.0 in stage 1.0 (TID 8). 1455 bytes result sent to driver
2018-11-19 14:10:32 TaskSetManager: INFO: Finished task 0.0 in stage 1.0 (TID 8) in 73 ms on localhost (executor driver) (3/3)
2018-11-19 14:10:32 TaskSchedulerImpl: INFO: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-11-19 14:10:32 DAGScheduler: INFO: ResultStage 1 (fold at RVD.scala:602) finished in 0.074 s
2018-11-19 14:10:32 DAGScheduler: INFO: Job 1 finished: fold at RVD.scala:602, took 0.087324 s
2018-11-19 14:11:03 SparkContext: INFO: Invoking stop() from shutdown hook
2018-11-19 14:11:03 AbstractConnector: INFO: Stopped Spark@f70b44e{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2018-11-19 14:11:03 SparkUI: INFO: Stopped Spark web UI at http://10.0.0.51:4041
2018-11-19 14:11:03 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!
2018-11-19 14:11:03 MemoryStore: INFO: MemoryStore cleared
2018-11-19 14:11:03 BlockManager: INFO: BlockManager stopped
2018-11-19 14:11:03 BlockManagerMaster: INFO: BlockManagerMaster stopped
2018-11-19 14:11:03 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!
2018-11-19 14:11:03 SparkContext: INFO: Successfully stopped SparkContext
2018-11-19 14:11:03 ShutdownHookManager: INFO: Shutdown hook called
2018-11-19 14:11:03 ShutdownHookManager: INFO: Deleting directory /private/var/folders/56/q7rh2y5x7hj4n359_13dgwxcmtwz20/T/spark-612728eb-3863-42c5-a775-7268c27b7ab4
2018-11-19 14:11:03 ShutdownHookManager: INFO: Deleting directory /private/var/folders/56/q7rh2y5x7hj4n359_13dgwxcmtwz20/T/spark-612728eb-3863-42c5-a775-7268c27b7ab4/pyspark-af77e137-8713-474a-b403-7961a5562c36
